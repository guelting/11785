{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy')\n",
    "fixtures_pred = np.load('../fixtures/dev_fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/dev_fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/test_fixtures/prediction.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/test_fixtures/generation.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "# Using the generator pattern (an iterable)\n",
    "class dataLoaderIter(object):\n",
    "    def __init__(self, dataLoader):\n",
    "        self.batch_num = dataLoader.batch_num\n",
    "        self.inputs = dataLoader.inputs\n",
    "        self.targets = dataLoader.targets\n",
    "        \n",
    "        self.ctr = 0\n",
    "        self.seq = 70\n",
    "        self.p = 0.95\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self.ctr < self.batch_num:\n",
    "\n",
    "            if np.random.random() < self.p:\n",
    "                seq_len = self.seq\n",
    "            else:\n",
    "                seq_len = self.seq / 2\n",
    "\n",
    "            seq_len = int(np.random.normal(loc=seq_len, scale=5))\n",
    "            cur, self.ctr = self.ctr, self.ctr + seq_len\n",
    "            \n",
    "            return (torch.tensor(self.inputs[cur : cur + seq_len]).long(),\n",
    "                    torch.tensor(self.targets[cur : cur + seq_len]).long())\n",
    "        \n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        #Randomly shuffle dataset\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.dataset)\n",
    "        self.dataset = np.concatenate(self.dataset)\n",
    "\n",
    "        #Find batch_num\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_num = len(self.dataset) // batch_size\n",
    "        \n",
    "        #Staggered inputs and ouputs\n",
    "        self.inputs = np.resize(self.dataset[:-1], (self.batch_size, self.batch_num)).T\n",
    "        self.targets = np.resize(self.dataset[1:], (self.batch_size, self.batch_num)).T\n",
    "        \n",
    "#         print(self.batch_size, self.batch_num)\n",
    "#         print(self.inputs.shape, self.targets.shape)\n",
    "#         print(dataset.shape, (self.dataset.shape))\n",
    "#         print(self.batch_num)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return dataLoaderIter(self)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for dataloader\n",
    "# loader = LanguageModelDataLoader(dataset=dataset, batch_size=32, shuffle=True)\n",
    "# for i in loader.__iter__():\n",
    "#     print (i[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for future... You know you should version control haha\n",
    "\n",
    "# class LanguageModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#         TODO: Define your model here\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self,vocab_size):\n",
    "#         super(LanguageModel,self).__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embed_size = 400\n",
    "#         self.hidden_size = 1150\n",
    "#         self.nlayers = 3\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, self.embed_size) # Embedding layer\n",
    "#         self.rnn = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=self.nlayers) # Recurrent network\n",
    "#         self.scoring = nn.Linear(self.hidden_size, self.vocab_size) # Projection layer\n",
    "        \n",
    "#     def forward(self,seq_batch,hidden): #L x N\n",
    "#         # returns 3D logits\n",
    "#         batch_size = seq_batch.size(1)\n",
    "#         embed = self.embedding(seq_batch) #L x N x E\n",
    "#         hidden = None\n",
    "#         output_lstm,hidden = self.rnn(embed, hidden) #L x N x H\n",
    "#         output_lstm_flatten = output_lstm.view(-1, self.hidden_size) #(L*N) x H\n",
    "#         output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
    "#         return output_flatten.view(-1, batch_size, self.vocab_size), hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        super(LanguageModel,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = 400\n",
    "        self.hidden_size = 1150\n",
    "        self.nlayers = 3\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.embed_size) # Embedding layer\n",
    "        self.rnns = [nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size),\n",
    "                     nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size),\n",
    "                     nn.LSTM(input_size=self.hidden_size, hidden_size=self.embed_size)]\n",
    "        self.rnn = nn.ModuleList(self.rnns) # LSTM\n",
    "        self.decoder = nn.Linear(self.embed_size, self.vocab_size) # Projection layer\n",
    "        \n",
    "        # Optimizations\n",
    "        tie_weight = False\n",
    "        if tie_weight:\n",
    "            self.encoder.weight = self.decoder.weight\n",
    "    \n",
    "    # hidden is made of two diff things apparently.. cell and hidden state\n",
    "    def forward(self,seq_batch, hidden): #L x N\n",
    "        # returns 3D logits\n",
    "        batch_size = seq_batch.size(1)\n",
    "        embed = self.encoder(seq_batch) #L x N x E\n",
    "        output_lstm = embed\n",
    "        new_hiddens = []\n",
    "        for i, l in enumerate(self.rnn):\n",
    "#             if hidden != None:\n",
    "#                 print (hidden[0].size(), hidden[1].size())\n",
    "            output_lstm, new_hidden = l(output_lstm, hidden[i])\n",
    "            new_hiddens.append(new_hidden)\n",
    "#             hidden = new_hidden\n",
    "        hidden = new_hiddens\n",
    "        output_lstm_flatten = output_lstm.view(-1, self.embed_size) #(L*N) x H\n",
    "        output_flatten = self.decoder(output_lstm_flatten) #(L*N) x V\n",
    "        return output_flatten.view(-1, batch_size, self.vocab_size), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return [(Variable(weight.new(1, batch_size, self.hidden_size).zero_()),\n",
    "                Variable(weight.new(1, batch_size, self.hidden_size).zero_())),\n",
    "               (Variable(weight.new(1, batch_size, self.hidden_size).zero_()),\n",
    "                Variable(weight.new(1, batch_size, self.hidden_size).zero_())),\n",
    "               (Variable(weight.new(1, batch_size, self.embed_size).zero_()),\n",
    "                Variable(weight.new(1, batch_size, self.embed_size).zero_()))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "# Temporary optim I don't want to implement ASGD\n",
    "import torch.optim as optim\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = self.criterion.to(DEVICE)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        print('starting training',time.strftime('%l:%M%p %Z on %b %d, %Y'))\n",
    "        start_time = time.time()\n",
    "        batch_start_time = start_time\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "            if batch_num % 10 == 0:\n",
    "                print(batch_num+1, epoch_loss/(batch_num+1))\n",
    "                curr_time = time.time()\n",
    "                print('Time elapsed:', time.strftime('%H:%M:%S', time.gmtime(curr_time - batch_start_time)))\n",
    "                batch_start_time = time.time()\n",
    "        \n",
    "        curr_time = time.time()\n",
    "        print('Time elapsed:', time.strftime('%H:%M:%S', time.gmtime(curr_time - start_time)))\n",
    "        \n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        hidden = self.model.init_hidden(BATCH_SIZE)\n",
    "        train_outputs = self.model(inputs,hidden)[0]\n",
    "        train_loss = self.criterion(train_outputs.view(-1,train_outputs.size(2)),targets.view(-1))\n",
    "        self.optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return train_loss.item()\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        \n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 20, self.model) # predictions for 20 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 20, self.model) # predictions for 20 words\n",
    "\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   NLL: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-test-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        # Apparently batch_size * sequence_length\n",
    "        # In my case seq length does not matter but batchsize does..\n",
    "        # The returned array should be [batch size, vocabulary size]\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        batch_size = inp.shape[0]\n",
    "        inp_tensor = torch.tensor(inp.T).long()\n",
    "        inp_tensor = inp_tensor.to(DEVICE)\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        val_outputs,_ = model(inp_tensor,hidden)\n",
    "        val_outputs = val_outputs.cpu().detach().numpy()\n",
    "        val_outputs = val_outputs[-1]\n",
    "#         print(val_outputs, val_outputs.shape)\n",
    "        return val_outputs\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"\n",
    "        generated_words = []\n",
    "        batch_size = inp.shape[0]\n",
    "\n",
    "        inp_tensor = torch.tensor(inp.T).long()\n",
    "        inp_tensor = inp_tensor.to(DEVICE)\n",
    "        \n",
    "#         embed = model.embedding(inp_tensor)\n",
    "#         hidden = None\n",
    "#         output_lstm, hidden = model.rnn(embed, hidden)\n",
    "#         output = output_lstm[-1]\n",
    "#         scores = model.scoring(output)\n",
    "#         _, current_word = torch.max(scores, dim=1)\n",
    "#         generated_words.append(current_word.unsqueeze(0))\n",
    "        \n",
    "#         if forward > 1 :\n",
    "#             for i in range(forward - 1):\n",
    "#                 embed = model.embedding(current_word).unsqueeze(0)\n",
    "#                 output_lstm, hidden = model.rnn(embed, hidden)\n",
    "#                 output = output_lstm[0]\n",
    "#                 scores = model.scoring(output)\n",
    "#                 _, current_word = torch.max(scores, dim=1)\n",
    "#                 generated_words.append((current_word.unsqueeze(0)))\n",
    "#         generated_words = torch.cat(generated_words, dim=0)\n",
    "#         return torch.Tensor.transpose(generated_words,0,1)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        output = inp_tensor\n",
    "        for i in range(forward):\n",
    "            scores, hidden_new = model(output, hidden)\n",
    "#             print(scores.size())\n",
    "            _, current_word = torch.max(scores, dim=2)\n",
    "#             print(current_word.size(),current_word[-1])\n",
    "            generated_words.append((current_word[-1].unsqueeze(0)))\n",
    "            output = current_word\n",
    "            hidden = hidden_new\n",
    "        generated_words = torch.cat(generated_words, dim=0)\n",
    "        res =  torch.Tensor.transpose(generated_words, 0, 1)\n",
    "#         print(res.size(),res[0])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1540492117\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "model = model.to(DEVICE)\n",
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training  9:15AM EDT on Oct 26, 2018\n",
      "1 10.409799575805664\n",
      "Time elapsed: 00:00:27\n",
      "11 8.80496866052801\n",
      "Time elapsed: 00:04:50\n",
      "21 8.125356537955147\n",
      "Time elapsed: 00:04:29\n",
      "31 7.894919210864652\n",
      "Time elapsed: 00:04:59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-cdafee235f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m  \u001b[0;31m# set to super large value at first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-e5479a4c25d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbatch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-e5479a4c25d5>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/11785/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/11785/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_nll = 1e30  # set to super large value at first\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \" + \n",
    "              str(epoch)+\" with NLL: \" + str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation NLL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | , , , , , , , , , , , , , , , , , , , ,\n",
      "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | , , , , , , , , , , , , , , , , , , , ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
